{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef219fa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.48\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.48      0.66      0.55       485\n",
      "           1       0.50      0.31      0.38       515\n",
      "\n",
      "    accuracy                           0.48      1000\n",
      "   macro avg       0.49      0.49      0.47      1000\n",
      "weighted avg       0.49      0.48      0.47      1000\n",
      "\n",
      "[0 1 1 ... 1 0 1]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Step 1: Load the Data\n",
    "train_data = pd.read_csv('macbook_twitter_sentiment_train.csv')\n",
    "test_data = pd.read_csv('macbook_twitter_sentiment_test.csv')\n",
    "\n",
    "# Step 2: Assign the correct target column name\n",
    "target_column = 'Label'  # Replace 'Label' with the correct column name for sentiment labels\n",
    "\n",
    "# Step 3: Feature Extraction and Data Preparation\n",
    "vectorizer = TfidfVectorizer(max_features=1000)\n",
    "X_train_all = vectorizer.fit_transform(train_data['Tweet'])\n",
    "y_train_all = train_data[target_column]\n",
    "\n",
    "# Splitting the training data into train and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_all, y_train_all, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 4: Train the Model\n",
    "model = XGBClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Step 5: Evaluate the Model on Validation Set\n",
    "y_pred_val = model.predict(X_val)\n",
    "accuracy_val = accuracy_score(y_val, y_pred_val)\n",
    "print(f\"Validation Accuracy: {accuracy_val:.2f}\")\n",
    "\n",
    "# Optional: Print classification report for detailed metrics on validation set\n",
    "print(classification_report(y_val, y_pred_val))\n",
    "\n",
    "# Step 6: Make Predictions on Test Data (without labels)\n",
    "X_test = vectorizer.transform(test_data['Tweet'])\n",
    "predictions_test = model.predict(X_test)\n",
    "print(predictions_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9be99da7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.48\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.48      0.66      0.55       485\n",
      "           1       0.50      0.31      0.38       515\n",
      "\n",
      "    accuracy                           0.48      1000\n",
      "   macro avg       0.49      0.49      0.47      1000\n",
      "weighted avg       0.49      0.48      0.47      1000\n",
      "\n",
      "[0 1 1 ... 1 0 1]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Step 1: Load the Data\n",
    "train_data = pd.read_csv('macbook_twitter_sentiment_train.csv')\n",
    "test_data = pd.read_csv('macbook_twitter_sentiment_test.csv')\n",
    "\n",
    "# Step 2: Assign the correct target column name\n",
    "target_column = 'Label'  # Replace 'Label' with the correct column name for sentiment labels\n",
    "\n",
    "# Step 3: Feature Extraction and Data Preparation\n",
    "vectorizer = TfidfVectorizer(max_features=1000)\n",
    "X_train_all = vectorizer.fit_transform(train_data['Tweet'])\n",
    "y_train_all = train_data[target_column]\n",
    "\n",
    "# Splitting the training data into train and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_all, y_train_all, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 4: Train the SVM Model\n",
    "svm_model = SVC(kernel='linear')  # You can specify different kernels and hyperparameters\n",
    "svm_model.fit(X_train, y_train)\n",
    "\n",
    "# Step 5: Evaluate the Model on Validation Set\n",
    "y_pred_val = svm_model.predict(X_val)\n",
    "accuracy_val = accuracy_score(y_val, y_pred_val)\n",
    "print(f\"Validation Accuracy: {accuracy_val:.2f}\")\n",
    "\n",
    "# Optional: Print classification report for detailed metrics on validation set\n",
    "print(classification_report(y_val, y_pred_val))\n",
    "\n",
    "# Step 6: Make Predictions on Test Data (without labels)\n",
    "X_test = vectorizer.transform(test_data['Tweet'])\n",
    "predictions_test = svm_model.predict(X_test)\n",
    "print(predictions_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed2c1643",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.48\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.48      0.66      0.55       485\n",
      "           1       0.50      0.31      0.38       515\n",
      "\n",
      "    accuracy                           0.48      1000\n",
      "   macro avg       0.49      0.49      0.47      1000\n",
      "weighted avg       0.49      0.48      0.47      1000\n",
      "\n",
      "[0 1 1 ... 1 0 1]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Step 1: Load the Data\n",
    "train_data = pd.read_csv('macbook_twitter_sentiment_train.csv')\n",
    "test_data = pd.read_csv('macbook_twitter_sentiment_test.csv')\n",
    "\n",
    "# Step 2: Assign the correct target column name\n",
    "target_column = 'Label'  # Replace 'Label' with the correct column name for sentiment labels\n",
    "\n",
    "# Step 3: Feature Extraction and Data Preparation\n",
    "vectorizer = TfidfVectorizer(max_features=1000)\n",
    "X_train_all = vectorizer.fit_transform(train_data['Tweet'])\n",
    "y_train_all = train_data[target_column]\n",
    "\n",
    "# Splitting the training data into train and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_all, y_train_all, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 4: Train the Logistic Regression Model\n",
    "logreg_model = LogisticRegression(max_iter=1000)  # You can adjust max_iter and other hyperparameters\n",
    "logreg_model.fit(X_train, y_train)\n",
    "\n",
    "# Step 5: Evaluate the Model on Validation Set\n",
    "y_pred_val = logreg_model.predict(X_val)\n",
    "accuracy_val = accuracy_score(y_val, y_pred_val)\n",
    "print(f\"Validation Accuracy: {accuracy_val:.2f}\")\n",
    "\n",
    "# Optional: Print classification report for detailed metrics on validation set\n",
    "print(classification_report(y_val, y_pred_val))\n",
    "\n",
    "# Step 6: Make Predictions on Test Data (without labels)\n",
    "X_test = vectorizer.transform(test_data['Tweet'])\n",
    "predictions_test = logreg_model.predict(X_test)\n",
    "print(predictions_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d8cc122f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.48\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.48      0.66      0.55       485\n",
      "           1       0.50      0.31      0.38       515\n",
      "\n",
      "    accuracy                           0.48      1000\n",
      "   macro avg       0.49      0.49      0.47      1000\n",
      "weighted avg       0.49      0.48      0.47      1000\n",
      "\n",
      "[0 1 1 ... 1 0 1]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Step 1: Load the Data\n",
    "train_data = pd.read_csv('macbook_twitter_sentiment_train.csv')\n",
    "test_data = pd.read_csv('macbook_twitter_sentiment_test.csv')\n",
    "\n",
    "# Step 2: Assign the correct target column name\n",
    "target_column = 'Label'  # Replace 'Label' with the correct column name for sentiment labels\n",
    "\n",
    "# Step 3: Feature Extraction and Data Preparation\n",
    "vectorizer = TfidfVectorizer(max_features=1000)\n",
    "X_train_all = vectorizer.fit_transform(train_data['Tweet'])\n",
    "y_train_all = train_data[target_column]\n",
    "\n",
    "# Splitting the training data into train and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_all, y_train_all, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 4: Train the Random Forest Model\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)  # You can adjust n_estimators and other hyperparameters\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Step 5: Evaluate the Model on Validation Set\n",
    "y_pred_val = rf_model.predict(X_val)\n",
    "accuracy_val = accuracy_score(y_val, y_pred_val)\n",
    "print(f\"Validation Accuracy: {accuracy_val:.2f}\")\n",
    "\n",
    "# Optional: Print classification report for detailed metrics on validation set\n",
    "print(classification_report(y_val, y_pred_val))\n",
    "\n",
    "# Step 6: Make Predictions on Test Data (without labels)\n",
    "X_test = vectorizer.transform(test_data['Tweet'])\n",
    "predictions_test = rf_model.predict(X_test)\n",
    "print(predictions_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c460a6e",
   "metadata": {},
   "source": [
    "xgboost tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ac48665f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 18 candidates, totalling 54 fits\n",
      "[CV 1/3] END learning_rate=0.1, max_depth=3, n_estimators=100;, score=0.532 total time=   0.0s\n",
      "[CV 2/3] END learning_rate=0.1, max_depth=3, n_estimators=100;, score=0.509 total time=   0.0s\n",
      "[CV 3/3] END learning_rate=0.1, max_depth=3, n_estimators=100;, score=0.536 total time=   0.0s\n",
      "[CV 1/3] END learning_rate=0.1, max_depth=3, n_estimators=200;, score=0.532 total time=   0.0s\n",
      "[CV 2/3] END learning_rate=0.1, max_depth=3, n_estimators=200;, score=0.509 total time=   0.0s\n",
      "[CV 3/3] END learning_rate=0.1, max_depth=3, n_estimators=200;, score=0.536 total time=   0.0s\n",
      "[CV 1/3] END learning_rate=0.1, max_depth=3, n_estimators=300;, score=0.532 total time=   0.0s\n",
      "[CV 2/3] END learning_rate=0.1, max_depth=3, n_estimators=300;, score=0.509 total time=   0.0s\n",
      "[CV 3/3] END learning_rate=0.1, max_depth=3, n_estimators=300;, score=0.536 total time=   0.0s\n",
      "[CV 1/3] END learning_rate=0.1, max_depth=5, n_estimators=100;, score=0.532 total time=   0.0s\n",
      "[CV 2/3] END learning_rate=0.1, max_depth=5, n_estimators=100;, score=0.509 total time=   0.0s\n",
      "[CV 3/3] END learning_rate=0.1, max_depth=5, n_estimators=100;, score=0.536 total time=   0.0s\n",
      "[CV 1/3] END learning_rate=0.1, max_depth=5, n_estimators=200;, score=0.532 total time=   0.0s\n",
      "[CV 2/3] END learning_rate=0.1, max_depth=5, n_estimators=200;, score=0.509 total time=   0.0s\n",
      "[CV 3/3] END learning_rate=0.1, max_depth=5, n_estimators=200;, score=0.536 total time=   0.0s\n",
      "[CV 1/3] END learning_rate=0.1, max_depth=5, n_estimators=300;, score=0.532 total time=   0.0s\n",
      "[CV 2/3] END learning_rate=0.1, max_depth=5, n_estimators=300;, score=0.509 total time=   0.0s\n",
      "[CV 3/3] END learning_rate=0.1, max_depth=5, n_estimators=300;, score=0.536 total time=   0.0s\n",
      "[CV 1/3] END learning_rate=0.1, max_depth=7, n_estimators=100;, score=0.532 total time=   0.0s\n",
      "[CV 2/3] END learning_rate=0.1, max_depth=7, n_estimators=100;, score=0.509 total time=   0.0s\n",
      "[CV 3/3] END learning_rate=0.1, max_depth=7, n_estimators=100;, score=0.536 total time=   0.0s\n",
      "[CV 1/3] END learning_rate=0.1, max_depth=7, n_estimators=200;, score=0.532 total time=   0.0s\n",
      "[CV 2/3] END learning_rate=0.1, max_depth=7, n_estimators=200;, score=0.509 total time=   0.0s\n",
      "[CV 3/3] END learning_rate=0.1, max_depth=7, n_estimators=200;, score=0.536 total time=   0.0s\n",
      "[CV 1/3] END learning_rate=0.1, max_depth=7, n_estimators=300;, score=0.532 total time=   0.0s\n",
      "[CV 2/3] END learning_rate=0.1, max_depth=7, n_estimators=300;, score=0.509 total time=   0.0s\n",
      "[CV 3/3] END learning_rate=0.1, max_depth=7, n_estimators=300;, score=0.536 total time=   0.0s\n",
      "[CV 1/3] END learning_rate=0.01, max_depth=3, n_estimators=100;, score=0.532 total time=   0.0s\n",
      "[CV 2/3] END learning_rate=0.01, max_depth=3, n_estimators=100;, score=0.509 total time=   0.0s\n",
      "[CV 3/3] END learning_rate=0.01, max_depth=3, n_estimators=100;, score=0.526 total time=   0.0s\n",
      "[CV 1/3] END learning_rate=0.01, max_depth=3, n_estimators=200;, score=0.532 total time=   0.0s\n",
      "[CV 2/3] END learning_rate=0.01, max_depth=3, n_estimators=200;, score=0.509 total time=   0.0s\n",
      "[CV 3/3] END learning_rate=0.01, max_depth=3, n_estimators=200;, score=0.536 total time=   0.0s\n",
      "[CV 1/3] END learning_rate=0.01, max_depth=3, n_estimators=300;, score=0.532 total time=   0.0s\n",
      "[CV 2/3] END learning_rate=0.01, max_depth=3, n_estimators=300;, score=0.509 total time=   0.0s\n",
      "[CV 3/3] END learning_rate=0.01, max_depth=3, n_estimators=300;, score=0.536 total time=   0.0s\n",
      "[CV 1/3] END learning_rate=0.01, max_depth=5, n_estimators=100;, score=0.532 total time=   0.0s\n",
      "[CV 2/3] END learning_rate=0.01, max_depth=5, n_estimators=100;, score=0.509 total time=   0.0s\n",
      "[CV 3/3] END learning_rate=0.01, max_depth=5, n_estimators=100;, score=0.526 total time=   0.0s\n",
      "[CV 1/3] END learning_rate=0.01, max_depth=5, n_estimators=200;, score=0.532 total time=   0.0s\n",
      "[CV 2/3] END learning_rate=0.01, max_depth=5, n_estimators=200;, score=0.509 total time=   0.0s\n",
      "[CV 3/3] END learning_rate=0.01, max_depth=5, n_estimators=200;, score=0.536 total time=   0.0s\n",
      "[CV 1/3] END learning_rate=0.01, max_depth=5, n_estimators=300;, score=0.532 total time=   0.1s\n",
      "[CV 2/3] END learning_rate=0.01, max_depth=5, n_estimators=300;, score=0.509 total time=   0.0s\n",
      "[CV 3/3] END learning_rate=0.01, max_depth=5, n_estimators=300;, score=0.536 total time=   0.1s\n",
      "[CV 1/3] END learning_rate=0.01, max_depth=7, n_estimators=100;, score=0.532 total time=   0.0s\n",
      "[CV 2/3] END learning_rate=0.01, max_depth=7, n_estimators=100;, score=0.509 total time=   0.0s\n",
      "[CV 3/3] END learning_rate=0.01, max_depth=7, n_estimators=100;, score=0.526 total time=   0.0s\n",
      "[CV 1/3] END learning_rate=0.01, max_depth=7, n_estimators=200;, score=0.532 total time=   0.0s\n",
      "[CV 2/3] END learning_rate=0.01, max_depth=7, n_estimators=200;, score=0.509 total time=   0.0s\n",
      "[CV 3/3] END learning_rate=0.01, max_depth=7, n_estimators=200;, score=0.536 total time=   0.0s\n",
      "[CV 1/3] END learning_rate=0.01, max_depth=7, n_estimators=300;, score=0.532 total time=   0.1s\n",
      "[CV 2/3] END learning_rate=0.01, max_depth=7, n_estimators=300;, score=0.509 total time=   0.1s\n",
      "[CV 3/3] END learning_rate=0.01, max_depth=7, n_estimators=300;, score=0.536 total time=   0.1s\n",
      "Best Parameters for XGBoost: {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 100}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Define the parameter grid for XGBoost\n",
    "param_grid_xgb = {\n",
    "    'learning_rate': [0.1, 0.01],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'n_estimators': [100, 200, 300]\n",
    "}\n",
    "\n",
    "# Initialize the XGBoost model\n",
    "xgb_model = XGBClassifier()\n",
    "\n",
    "# Perform Grid Search to find the best parameters\n",
    "grid_search_xgb = GridSearchCV(estimator=xgb_model, param_grid=param_grid_xgb, cv=3, scoring='accuracy',  verbose=3)\n",
    "grid_search_xgb.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameters and the best estimator\n",
    "best_params_xgb = grid_search_xgb.best_params_\n",
    "best_estimator_xgb = grid_search_xgb.best_estimator_\n",
    "\n",
    "print(\"Best Parameters for XGBoost:\", best_params_xgb)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f9f14e",
   "metadata": {},
   "source": [
    "svm tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "07f0ec93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 6 candidates, totalling 18 fits\n",
      "[CV 1/3] END ..............C=0.1, kernel=linear;, score=0.532 total time=   0.3s\n",
      "[CV 2/3] END ..............C=0.1, kernel=linear;, score=0.509 total time=   0.3s\n",
      "[CV 3/3] END ..............C=0.1, kernel=linear;, score=0.526 total time=   0.3s\n",
      "[CV 1/3] END .................C=0.1, kernel=rbf;, score=0.532 total time=   0.3s\n",
      "[CV 2/3] END .................C=0.1, kernel=rbf;, score=0.509 total time=   0.3s\n",
      "[CV 3/3] END .................C=0.1, kernel=rbf;, score=0.526 total time=   0.3s\n",
      "[CV 1/3] END ................C=1, kernel=linear;, score=0.532 total time=   0.2s\n",
      "[CV 2/3] END ................C=1, kernel=linear;, score=0.509 total time=   0.2s\n",
      "[CV 3/3] END ................C=1, kernel=linear;, score=0.536 total time=   0.2s\n",
      "[CV 1/3] END ...................C=1, kernel=rbf;, score=0.532 total time=   0.3s\n",
      "[CV 2/3] END ...................C=1, kernel=rbf;, score=0.509 total time=   0.2s\n",
      "[CV 3/3] END ...................C=1, kernel=rbf;, score=0.536 total time=   0.3s\n",
      "[CV 1/3] END ...............C=10, kernel=linear;, score=0.532 total time=   0.3s\n",
      "[CV 2/3] END ...............C=10, kernel=linear;, score=0.509 total time=   0.2s\n",
      "[CV 3/3] END ...............C=10, kernel=linear;, score=0.536 total time=   0.2s\n",
      "[CV 1/3] END ..................C=10, kernel=rbf;, score=0.532 total time=   0.3s\n",
      "[CV 2/3] END ..................C=10, kernel=rbf;, score=0.509 total time=   0.2s\n",
      "[CV 3/3] END ..................C=10, kernel=rbf;, score=0.536 total time=   0.3s\n",
      "Best Parameters for SVM: {'C': 1, 'kernel': 'linear'}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Define the parameter grid for SVM\n",
    "param_grid_svm = {\n",
    "    'C': [0.1, 1, 10],\n",
    "    'kernel': ['linear', 'rbf']\n",
    "}\n",
    "\n",
    "# Initialize the SVM model\n",
    "svm_model = SVC()\n",
    "\n",
    "# Perform Grid Search to find the best parameters\n",
    "grid_search_svm = GridSearchCV(estimator=svm_model, param_grid=param_grid_svm, cv=3, scoring='accuracy',  verbose=3)\n",
    "grid_search_svm.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameters and the best estimator\n",
    "best_params_svm = grid_search_svm.best_params_\n",
    "best_estimator_svm = grid_search_svm.best_estimator_\n",
    "\n",
    "print(\"Best Parameters for SVM:\", best_params_svm)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab1bfb1",
   "metadata": {},
   "source": [
    "Logistic Regression tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "84bf53f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 6 candidates, totalling 18 fits\n",
      "[CV 1/3] END ...................C=0.1, penalty=l1;, score=nan total time=   0.0s\n",
      "[CV 2/3] END ...................C=0.1, penalty=l1;, score=nan total time=   0.0s\n",
      "[CV 3/3] END ...................C=0.1, penalty=l1;, score=nan total time=   0.0s\n",
      "[CV 1/3] END .................C=0.1, penalty=l2;, score=0.532 total time=   0.0s\n",
      "[CV 2/3] END .................C=0.1, penalty=l2;, score=0.509 total time=   0.0s\n",
      "[CV 3/3] END .................C=0.1, penalty=l2;, score=0.536 total time=   0.0s\n",
      "[CV 1/3] END .....................C=1, penalty=l1;, score=nan total time=   0.0s\n",
      "[CV 2/3] END .....................C=1, penalty=l1;, score=nan total time=   0.0s\n",
      "[CV 3/3] END .....................C=1, penalty=l1;, score=nan total time=   0.0s\n",
      "[CV 1/3] END ...................C=1, penalty=l2;, score=0.532 total time=   0.0s\n",
      "[CV 2/3] END ...................C=1, penalty=l2;, score=0.509 total time=   0.0s\n",
      "[CV 3/3] END ...................C=1, penalty=l2;, score=0.536 total time=   0.0s\n",
      "[CV 1/3] END ....................C=10, penalty=l1;, score=nan total time=   0.0s\n",
      "[CV 2/3] END ....................C=10, penalty=l1;, score=nan total time=   0.0s\n",
      "[CV 3/3] END ....................C=10, penalty=l1;, score=nan total time=   0.0s\n",
      "[CV 1/3] END ..................C=10, penalty=l2;, score=0.532 total time=   0.0s\n",
      "[CV 2/3] END ..................C=10, penalty=l2;, score=0.509 total time=   0.0s\n",
      "[CV 3/3] END ..................C=10, penalty=l2;, score=0.536 total time=   0.0s\n",
      "Best Parameters for Logistic Regression: {'C': 0.1, 'penalty': 'l2'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jayan\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:372: FitFailedWarning: \n",
      "9 fits failed out of a total of 18.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "9 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\jayan\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\jayan\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1461, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\jayan\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 447, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Users\\jayan\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:969: UserWarning: One or more of the test scores are non-finite: [       nan 0.52574838        nan 0.52574838        nan 0.52574838]\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Define the parameter grid for Logistic Regression\n",
    "param_grid_logreg = {\n",
    "    'C': [0.1, 1, 10],\n",
    "    'penalty': ['l1', 'l2']\n",
    "}\n",
    "\n",
    "# Initialize the Logistic Regression model\n",
    "logreg_model = LogisticRegression(max_iter=1000)\n",
    "\n",
    "# Perform Grid Search to find the best parameters\n",
    "grid_search_logreg = GridSearchCV(estimator=logreg_model, param_grid=param_grid_logreg, cv=3, scoring='accuracy',  verbose=3)\n",
    "grid_search_logreg.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameters and the best estimator\n",
    "best_params_logreg = grid_search_logreg.best_params_\n",
    "best_estimator_logreg = grid_search_logreg.best_estimator_\n",
    "\n",
    "print(\"Best Parameters for Logistic Regression:\", best_params_logreg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "419c370c",
   "metadata": {},
   "source": [
    "# RF tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1e02a066",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 12 candidates, totalling 36 fits\n",
      "[CV 1/3] END ..max_depth=None, n_estimators=100;, score=0.518 total time=   0.1s\n",
      "[CV 2/3] END ..max_depth=None, n_estimators=100;, score=0.509 total time=   0.1s\n",
      "[CV 3/3] END ..max_depth=None, n_estimators=100;, score=0.536 total time=   0.1s\n",
      "[CV 1/3] END ..max_depth=None, n_estimators=200;, score=0.532 total time=   0.3s\n",
      "[CV 2/3] END ..max_depth=None, n_estimators=200;, score=0.509 total time=   0.3s\n",
      "[CV 3/3] END ..max_depth=None, n_estimators=200;, score=0.536 total time=   0.3s\n",
      "[CV 1/3] END ..max_depth=None, n_estimators=300;, score=0.532 total time=   0.4s\n",
      "[CV 2/3] END ..max_depth=None, n_estimators=300;, score=0.509 total time=   0.4s\n",
      "[CV 3/3] END ..max_depth=None, n_estimators=300;, score=0.536 total time=   0.4s\n",
      "[CV 1/3] END .....max_depth=5, n_estimators=100;, score=0.518 total time=   0.0s\n",
      "[CV 2/3] END .....max_depth=5, n_estimators=100;, score=0.509 total time=   0.0s\n",
      "[CV 3/3] END .....max_depth=5, n_estimators=100;, score=0.536 total time=   0.0s\n",
      "[CV 1/3] END .....max_depth=5, n_estimators=200;, score=0.532 total time=   0.2s\n",
      "[CV 2/3] END .....max_depth=5, n_estimators=200;, score=0.509 total time=   0.2s\n",
      "[CV 3/3] END .....max_depth=5, n_estimators=200;, score=0.536 total time=   0.2s\n",
      "[CV 1/3] END .....max_depth=5, n_estimators=300;, score=0.532 total time=   0.3s\n",
      "[CV 2/3] END .....max_depth=5, n_estimators=300;, score=0.509 total time=   0.3s\n",
      "[CV 3/3] END .....max_depth=5, n_estimators=300;, score=0.536 total time=   0.3s\n",
      "[CV 1/3] END ....max_depth=10, n_estimators=100;, score=0.532 total time=   0.1s\n",
      "[CV 2/3] END ....max_depth=10, n_estimators=100;, score=0.509 total time=   0.1s\n",
      "[CV 3/3] END ....max_depth=10, n_estimators=100;, score=0.536 total time=   0.1s\n",
      "[CV 1/3] END ....max_depth=10, n_estimators=200;, score=0.532 total time=   0.2s\n",
      "[CV 2/3] END ....max_depth=10, n_estimators=200;, score=0.509 total time=   0.2s\n",
      "[CV 3/3] END ....max_depth=10, n_estimators=200;, score=0.536 total time=   0.3s\n",
      "[CV 1/3] END ....max_depth=10, n_estimators=300;, score=0.532 total time=   0.4s\n",
      "[CV 2/3] END ....max_depth=10, n_estimators=300;, score=0.509 total time=   0.4s\n",
      "[CV 3/3] END ....max_depth=10, n_estimators=300;, score=0.536 total time=   0.4s\n",
      "[CV 1/3] END ....max_depth=15, n_estimators=100;, score=0.532 total time=   0.1s\n",
      "[CV 2/3] END ....max_depth=15, n_estimators=100;, score=0.509 total time=   0.1s\n",
      "[CV 3/3] END ....max_depth=15, n_estimators=100;, score=0.536 total time=   0.1s\n",
      "[CV 1/3] END ....max_depth=15, n_estimators=200;, score=0.532 total time=   0.2s\n",
      "[CV 2/3] END ....max_depth=15, n_estimators=200;, score=0.509 total time=   0.2s\n",
      "[CV 3/3] END ....max_depth=15, n_estimators=200;, score=0.536 total time=   0.2s\n",
      "[CV 1/3] END ....max_depth=15, n_estimators=300;, score=0.532 total time=   0.6s\n",
      "[CV 2/3] END ....max_depth=15, n_estimators=300;, score=0.509 total time=   0.5s\n",
      "[CV 3/3] END ....max_depth=15, n_estimators=300;, score=0.536 total time=   0.5s\n",
      "Best Parameters for Random Forest: {'max_depth': None, 'n_estimators': 200}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Define the parameter grid for Random Forest\n",
    "param_grid_rf = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [None, 5, 10, 15]\n",
    "}\n",
    "\n",
    "# Initialize the Random Forest model\n",
    "rf_model = RandomForestClassifier()\n",
    "\n",
    "# Perform Grid Search to find the best parameters\n",
    "grid_search_rf = GridSearchCV(estimator=rf_model, param_grid=param_grid_rf, cv=3, scoring='accuracy',  verbose=3)\n",
    "grid_search_rf.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameters and the best estimator\n",
    "best_params_rf = grid_search_rf.best_params_\n",
    "best_estimator_rf = grid_search_rf.best_estimator_\n",
    "\n",
    "print(\"Best Parameters for Random Forest:\", best_params_rf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "700e3e0d",
   "metadata": {},
   "source": [
    "# overall sentiment score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eef30fa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Sentiment Score:\n",
      "[0. 1. 1. ... 1. 0. 1.]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Using each best estimator to predict on the test data\n",
    "predictions_test_xgb = best_estimator_xgb.predict(X_test)\n",
    "predictions_test_svm = best_estimator_svm.predict(X_test)\n",
    "predictions_test_logreg = best_estimator_logreg.predict(X_test)\n",
    "predictions_test_rf = best_estimator_rf.predict(X_test)\n",
    "\n",
    "# Creating an array of predictions for each model\n",
    "all_predictions = np.array([\n",
    "    predictions_test_xgb,\n",
    "    predictions_test_svm,\n",
    "    predictions_test_logreg,\n",
    "    predictions_test_rf\n",
    "])\n",
    "\n",
    "# Calculating the overall sentiment scores of each fine tuned  models\n",
    "overall_sentiment_score = np.mean(all_predictions, axis=0)\n",
    "print(\"Overall Sentiment Score:\")\n",
    "print(overall_sentiment_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6654e736",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Sentiment Score (Proportion of Positive Sentiments): 0.3016\n"
     ]
    }
   ],
   "source": [
    "#Calculating the overall sentiment score of the ensemble model\n",
    "overall_sentiment_score = np.mean(overall_sentiment_score)  # Calculating the mean of all predictions (proportion of positive sentiments)\n",
    "print(\"Overall Sentiment Score (Proportion of Positive Sentiments):\", overall_sentiment_score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f93e5d",
   "metadata": {},
   "source": [
    "the overall sentiment leans slightly towards the positive side, but it's not overwhelmingly positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d179256e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 Positive Tweets with Positive Words:\n",
      "The new MacBook Pro is incredibly fast and efficient!\n",
      "MacBook's retina display is stunning, great for design work.\n",
      "The new MacBook Pro is incredibly fast and efficient!\n",
      "Switching to MacBook has been a great experience, so user-friendly.\n",
      "MacBook's retina display is stunning, great for design work.\n",
      "\n",
      "Top 5 Negative Tweets with Negative Words:\n",
      "Battery life on the latest MacBook is a bad.\n",
      "The touch bar on the MacBook Pro is a game changer.\n",
      "Battery life on the latest MacBook is a bad.\n",
      "My MacBook keeps overheating, not what I expected.\n",
      "Frustrated with the lack of ports on my MacBook.\n"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "from collections import Counter\n",
    "\n",
    "# Function to get sentiment polarity for a text\n",
    "def get_sentiment_polarity(text):\n",
    "    return TextBlob(text).sentiment.polarity\n",
    "\n",
    "# Calculate sentiment polarity for each tweet in the test dataset\n",
    "test_data['Sentiment_Polarity'] = test_data['Tweet'].apply(get_sentiment_polarity)\n",
    "\n",
    "# Separate tweets based on their sentiment polarity\n",
    "positive_tweets = test_data[test_data['Sentiment_Polarity'] > 0]['Tweet']\n",
    "negative_tweets = test_data[test_data['Sentiment_Polarity'] < 0]['Tweet']\n",
    "\n",
    "# Function to extract words from tweets\n",
    "def extract_words(tweets):\n",
    "    words = []\n",
    "    for tweet in tweets:\n",
    "        words.extend(tweet.split())\n",
    "    return words\n",
    "\n",
    "# Extract positive and negative words from the dataset\n",
    "positive_words = extract_words(positive_tweets)\n",
    "negative_words = extract_words(negative_tweets)\n",
    "\n",
    "# Get the most common positive and negative words\n",
    "most_common_positive = Counter(positive_words).most_common(10)\n",
    "most_common_negative = Counter(negative_words).most_common(10)\n",
    "\n",
    "# Function to find tweets containing certain words\n",
    "def find_tweets_with_words(tweets, words):\n",
    "    selected_tweets = []\n",
    "    for tweet in tweets:\n",
    "        if any(word in tweet for word in words):\n",
    "            selected_tweets.append(tweet)\n",
    "    return selected_tweets\n",
    "\n",
    "# Find positive and negative tweets containing the most common positive and negative words\n",
    "positive_tweets_containing_words = find_tweets_with_words(positive_tweets, [word[0] for word in most_common_positive])\n",
    "negative_tweets_containing_words = find_tweets_with_words(negative_tweets, [word[0] for word in most_common_negative])\n",
    "\n",
    "# Print 5 positive tweets containing positive words\n",
    "print(\"Top 5 Positive Tweets with Positive Words:\")\n",
    "for tweet in positive_tweets_containing_words[:5]:\n",
    "    print(tweet)\n",
    "\n",
    "# Print 5 negative tweets containing negative words\n",
    "print(\"\\nTop 5 Negative Tweets with Negative Words:\")\n",
    "for tweet in negative_tweets_containing_words[:5]:\n",
    "    print(tweet)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1174b959",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d76092a2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
